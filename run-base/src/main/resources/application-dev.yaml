server:
  port: 8007

logging:
  level:
    org.apache.hudi: INFO
    org.apache.kafka: ERROR
    io.confluent.kafka: ERROR
    org.apache.hudi.multistream: DEBUG
    org.hibernate.SQL: DEBUG
    org.hibernate.type.descriptor.sql.BasicBinder: TRACE
    org.hibernate.type.descriptor.sql.BasicExtractor: TRACE

spring:
  application:
    name: hudi-multistream-bigdata
  datasource:
    driver-class-name: com.mysql.jdbc.Driver
    url: jdbc:mysql://xxxx:3306/bigdata
    username: xxxx
    password: xxxx
  jpa:
    properties:
      hibernate:
        format_sql: true
  cloud:
    consul:
      discovery:
        tags: tag-bigdata-mysql,bigdata-mysql
  kafka:
    bootstrap-servers: kafka-node:9092
    properties:
      schema.registry.url: http://schema-registry-node:8081
    consumer:
      group-id: hoodie-delta-streamer
      key-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      enable-auto-commit: false
      auto-offset-reset: earliest

hudi:
  properties:
    hoodie.embed.timeline.server: false
    hoodie.filesystem.view.remote.host: xxxx
    hoodie.filesystem.view.remote.port: 26754
    hoodie.filesystem.view.type: REMOTE_ONLY
    hoodie.upsert.shuffle.parallelism: 2
    hoodie.insert.shuffle.parallelism: 2
    hoodie.bulkinsert.shuffle.parallelism: 2
    hoodie.datasource.write.recordkey.field: _row_key
    hoodie.datasource.write.partitionpath.field:
    hoodie.datasource.write.precombine.field: _extend_ts_ms
    hoodie.datasource.write.keygenerator.class: org.apache.hudi.keygen.NonpartitionedKeyGenerator
    hoodie.datasource.hive_sync.partition_extractor_class: org.apache.hudi.hive.NonPartitionedExtractor
    hoodie.datasource.hive_sync.use_jdbc: true
    hoodie.datasource.hive_sync.username: xxxx
    hoodie.datasource.hive_sync.password: xxxx
    hoodie.datasource.hive_sync.jdbcurl: jdbc:hive2://hive-node:10000
    hoodie.write.commit.callback.on: true
    hoodie.write.commit.callback.class: org.apache.hudi.multistream.hudi.callback.HudiWriteCommitCallback
  delta:
    config-folder: file:///xxxx/delta_props/
    table-type: MERGE_ON_READ
    source-class: org.apache.hudi.utilities.sources.AvroKafkaSource
    source-ordering-field: _extend_ts_ms
    schemaprovider-class: xxxx
    transformer-class: xxxx
    op: UPSERT
    enable-hive-sync: true
    max-pending-compactions:
    spark-master: local[1]
    payload-class: xxxx
  kafka:
    topic: bigdata-mysql.bigdata.(.*)
    # FILE, TOPIC, DB
    base-on: DB
    # loop interval(second)
    loop-interval: 5
  zookeeper:
    connect: xxxx:2181
    path: hudi
    client:
      session-timeout: 5000
      connection-timeout: 5000
      base-sleep-timeout: 1000
      max-retries: 3
  base-path: /hive/warehouse/%s.db/%s
  hadoop:
    kerberos:
      enable: false