server:
  port: 8007

spring:
  application:
    name: hudi-multistream-bigdata
  datasource:
    driver-class-name: com.mysql.jdbc.Driver
    url: jdbc:mysql://xxxx:3306/bigdata
    username: xxxx
    password: xxxx
  jpa:
    properties:
      hibernate:
        format_sql: true
        jdbc:
          batch_size: 100
        order_inserts: true
        generate_statistics: true
  #    show-sql: true
  cloud:
    consul:
      host: ${CONSUL_HOST:localhost}
      port: 8500
      discovery:
        service-name: ${spring.application.name}
        prefer-ip-address: true
        healthCheckInterval: 5s
        port: ${server.port}
        tags: tag-bigdata-mysql,bigdata-mysql
  kafka:
    bootstrap-servers: kafka-node:9092
    properties:
      schema.registry.url: http://schema-registry-node:8081
    consumer:
      group-id: hoodie-delta-streamer
      key-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      enable-auto-commit: false

hudi:
  properties:
    hoodie.embed.timeline.server: false
    hoodie.filesystem.view.remote.host: xxxx
    hoodie.filesystem.view.remote.port: 26754
    hoodie.filesystem.view.type: REMOTE_ONLY
    hoodie.upsert.shuffle.parallelism: 2
    hoodie.insert.shuffle.parallelism: 2
    hoodie.bulkinsert.shuffle.parallelism: 2
    hoodie.datasource.write.recordkey.field: _row_key
    hoodie.datasource.write.partitionpath.field:
    hoodie.datasource.write.precombine.field: _extend_ts_ms
    hoodie.datasource.write.keygenerator.class: org.apache.hudi.keygen.NonpartitionedKeyGenerator
    hoodie.datasource.hive_sync.partition_extractor_class: org.apache.hudi.hive.NonPartitionedExtractor
    hoodie.datasource.hive_sync.use_jdbc: false
    hoodie.datasource.hive_sync.username: xxxx
    hoodie.datasource.hive_sync.password: xxxx
    hoodie.datasource.hive_sync.jdbcurl: jdbc:hive2://hive-node:10000
    ods.deltastreamer.source.table.pk: uid
  delta:
    config-folder: file:///xxxx/delta_props/
    table-type: MERGE_ON_READ
    source-class: org.apache.hudi.utilities.sources.AvroKafkaSource
    source-ordering-field: _extend_deal_date
    schemaprovider-class: xxxx
    transformer-class: xxxx
    op: UPSERT
    enable-hive-sync: true
    commit-on-errors: false
    disable-compaction: false
  kafka:
    topic: bigdata-mysql.bigdata.(.*)
    # FILE, TOPIC
    base-on: TOPIC
    loop-interval: 10
  zookeeper:
    connect: xxxx:2181
    path: hudi
    client:
      session-timeout: 5000
      connection-timeout: 5000
      base-sleep-timeout: 1000
      max-retries: 3
  base-path: /hive/warehouse/%s.db/%s